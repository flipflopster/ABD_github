{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969ccf3a",
   "metadata": {},
   "source": [
    "## Reading the previously analized and prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13594bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LinearSVC, LogisticRegression, GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10128e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 42366)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Build SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimesFix\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0428f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "data_dir = '../Datasets/'\n",
    "file_crimes = data_dir + '3_crimes_cleaned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb14590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = spark.read.parquet(file_crimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "846fc3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_clean - number of rows: 7474272\n",
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- IUCR_Num: integer (nullable = true)\n",
      " |-- Primary_Type_Num: integer (nullable = true)\n",
      " |-- Location_Description_Num: integer (nullable = true)\n",
      " |-- Arrest: integer (nullable = true)\n",
      " |-- Domestic: integer (nullable = true)\n",
      " |-- Beat: integer (nullable = true)\n",
      " |-- District: integer (nullable = true)\n",
      " |-- Ward: integer (nullable = true)\n",
      " |-- Community_Area: integer (nullable = true)\n",
      " |-- FBI_Code_Num: integer (nullable = true)\n",
      "\n",
      "+----+-----+---+----+------+--------+----------------+------------------------+------+--------+----+--------+----+--------------+------------+\n",
      "|Year|Month|Day|Hour|Minute|IUCR_Num|Primary_Type_Num|Location_Description_Num|Arrest|Domestic|Beat|District|Ward|Community_Area|FBI_Code_Num|\n",
      "+----+-----+---+----+------+--------+----------------+------------------------+------+--------+----+--------+----+--------------+------------+\n",
      "|2003|    2| 16|  14|    38|      48|               7|                       0|     0|       0|2514|      25|  31|            19|           7|\n",
      "|2001|    1| 11|   4|    30|      77|              21|                       2|     1|       1| 333|       3|   5|            43|          22|\n",
      "|2001|    1| 19|   1|     7|      77|              21|                       0|     1|       0|1724|      17|  33|            16|          22|\n",
      "|2001|    2|  5|  13|    45|      77|              21|                     141|     1|       0|2412|      24|  50|             2|          22|\n",
      "|2001|    2|  2|   8|    27|      15|               3|                       0|     1|       0|1523|      15|  29|            25|           4|\n",
      "|2001|    2|  3|  18|    52|      44|               7|                       0|     0|       0|1134|      11|  28|            27|           7|\n",
      "|2001|    2|  3|  21|    22|      10|               3|                       4|     1|       0|1121|      11|  26|            23|           4|\n",
      "|2001|    2|  2|   6|    30|       7|               7|                       0|     0|       0|2511|      25|  36|            19|           7|\n",
      "|2001|    2| 11|   8|    30|      77|              21|                      91|     1|       0|1531|      15|  28|            25|          22|\n",
      "|2001|    2| 13|   7|    45|       7|               7|                       0|     0|       0| 815|       8|  23|            56|           7|\n",
      "+----+-----+---+----+------+--------+----------------+------------------------+------+--------+----+--------+----+--------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data\n",
    "print(f'df_clean - number of rows: {df_clean.count()}')\n",
    "df_clean = df_clean.drop('IUCR', 'Primary_Type', 'Location_Description', 'FBI_Code')\n",
    "df_clean.printSchema()\n",
    "df_clean.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890f80f7",
   "metadata": {},
   "source": [
    "Since we already indexed the relevant categorical columns, we can skip the String Indexer phase of the pipeline we are creating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da5e54f",
   "metadata": {},
   "source": [
    "The following columns were already indexed the previous notebook:\n",
    "\n",
    "IUCR_Num\n",
    "\n",
    "Primary_Type_Num\n",
    "\n",
    "Location_Description_Num\n",
    "\n",
    "FBI_Code_Num_Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c77b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_categorical = ['IUCR_Num', 'Primary_Type_Num', 'Location_Description_Num', 'Arrest', 'Domestic', 'Beat',  'District', 'Ward', 'Community_Area', 'FBI_Code_Num']\n",
    "\n",
    "cols_numeric = [col for col in df_clean.columns if col not in cols_categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b7ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['IUCR_Num', 'Primary_Type_Num', 'Location_Description_Num', 'Arrest', 'Domestic', 'Beat', 'District', 'Ward', 'Community_Area', 'FBI_Code_Num']\n",
      "Numeric columns: ['Year', 'Month', 'Day', 'Hour', 'Minute']\n"
     ]
    }
   ],
   "source": [
    "print(f'Categorical columns: {cols_categorical}')\n",
    "print(f'Numeric columns: {cols_numeric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff317009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['IUCR_Num', 'Primary_Type_Num', 'Location_Description_Num', 'Domestic', 'Beat', 'District', 'Ward', 'Community_Area', 'FBI_Code_Num']\n"
     ]
    }
   ],
   "source": [
    "cols_not_features = ['Arrest']\n",
    "\n",
    "\n",
    "categorical_cols = [i for i in cols_categorical if i not in cols_not_features]\n",
    "print(f'Categorical columns: {categorical_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37e3832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "non_categorical_cols = [i for i in cols_numeric if i not in cols_not_features]\n",
    "ohe_output_cols = [x + ' OHE' for x in categorical_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ee829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembler inputs: ['IUCR_Num OHE', 'Primary_Type_Num OHE', 'Location_Description_Num OHE', 'Domestic OHE', 'Beat OHE', 'District OHE', 'Ward OHE', 'Community_Area OHE', 'FBI_Code_Num OHE', 'Year', 'Month', 'Day', 'Hour', 'Minute']\n"
     ]
    }
   ],
   "source": [
    "ohe_encoder = OneHotEncoder(inputCols=categorical_cols, outputCols=ohe_output_cols, handleInvalid=\"keep\")\n",
    "assembler_inputs = ohe_output_cols + non_categorical_cols\n",
    "print(f'Assembler inputs: {assembler_inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9221d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ae71b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5232322 rows in the training set and 2241950 rows in the validation set.\n"
     ]
    }
   ],
   "source": [
    "df_train, df_validation = df_clean.randomSplit([0.7, 0.3], 42)\n",
    "\n",
    "print(f'There are {df_train.count()} rows in the training set and {df_validation.count()} rows in the validation set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfbc61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.write.mode('overwrite').parquet('../Datasets/crimes-small-train')\n",
    "df_validation.write.mode('overwrite').parquet('../Datasets/crimes-small-validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5402da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_clean' in locals():\n",
    "    del df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f08772",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(maxIter=10, regParam=0.1, labelCol='Arrest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "436de977",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[ohe_encoder, vec_assembler, lsvc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84a22471",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.write().overwrite().save('../Datasets/pipeline-LinearSVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb3afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_rows = 100000\n",
    "model = pipeline.fit(df_train.limit(limit_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1140527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save('model-LinearSVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a00422",
   "metadata": {},
   "source": [
    "## Lets make more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ebf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression Classifier\n",
    "logreg = LogisticRegression(maxIter=10, regParam=0.1, labelCol='Arrest')\n",
    "pipeline_logreg = Pipeline(stages=[ohe_encoder, vec_assembler, logreg])\n",
    "pipeline_logreg.write().overwrite().save('../Datasets/pipeline-LogReg')\n",
    "model_logreg = pipeline_logreg.fit(df_train.limit(limit_rows))\n",
    "model_logreg.write().overwrite().save('model-LogReg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(labelCol='Arrest', featuresCol='features', numTrees=20)\n",
    "pipeline_rf = Pipeline(stages=[ohe_encoder, vec_assembler, rf])\n",
    "pipeline_rf.write().overwrite().save('../Datasets/pipeline-RandomForest')\n",
    "model_rf = pipeline_rf.fit(df_train.limit(limit_rows))\n",
    "model_rf.write().overwrite().save('model-RandomForest')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
