{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce7ac0a",
   "metadata": {},
   "source": [
    "# Data classification using ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba88119",
   "metadata": {},
   "source": [
    "In this notebooke we will use the ML models we saved in the previous notebook to classify batches of rows and figure out the optimal model for our data.\n",
    "\n",
    "Afterwards, we'll use the winning model for the data classification in a stream format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb4e1389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LinearSVC, LogisticRegression, GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ac9f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimesFix\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ed0004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = spark.read.parquet(\"../Datasets/crimes-small-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8d9c7",
   "metadata": {},
   "source": [
    "### Lets start evaluating the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12e10621",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linear = PipelineModel.load('model-LinearSVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca71ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric areaUnderROC of the linear model = 0.8672023696346972\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_predictions = model_linear.transform(df_validation)\n",
    "\n",
    "df_predictions_eval = df_predictions.select('features', \n",
    "                    'rawPrediction', 'prediction', 'Arrest')\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol='Arrest',\n",
    "                                                rawPredictionCol='rawPrediction',\n",
    "                                                metricName='areaUnderROC')\n",
    "    \n",
    "area_under_ROC_linear = binary_evaluator.evaluate(df_predictions_eval)\n",
    "\n",
    "# Print out result\n",
    "print(f'Metric areaUnderROC of the linear model = {area_under_ROC_linear}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38926aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+\n",
      "|prediction|Arrest|  count|\n",
      "+----------+------+-------+\n",
      "|       0.0|     0|1596941|\n",
      "|       1.0|     1| 368949|\n",
      "|       1.0|     0|  77815|\n",
      "|       0.0|     1| 198245|\n",
      "+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting of the kind of predictions made\n",
    "df_confusion_matrix = df_predictions_eval.groupBy('prediction', 'Arrest').count()\n",
    "df_confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6ba4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "tp = df_confusion_matrix.filter((F.col('prediction')==1.0) & (F.col('Arrest')==1)).first()\n",
    "tn = df_confusion_matrix.filter((F.col('prediction')==0.0) & (F.col('Arrest')==0)).first()\n",
    "fp = df_confusion_matrix.filter((F.col('prediction')==1.0) & (F.col('Arrest')==0)).first()\n",
    "fn = df_confusion_matrix.filter((F.col('prediction')==0.0) & (F.col('Arrest')==1)).first()\n",
    "\n",
    "confmat = {'TP': 0.0, 'TN': 0.0, 'FP': 0.0, 'FN': 0.0}\n",
    "if (tp):\n",
    "    confmat['TP'] = tp['count'] * 1.0\n",
    "if (tn):\n",
    "    confmat['TN'] = tn['count'] * 1.0\n",
    "if (fp):\n",
    "    confmat['FP'] = fp['count'] * 1.0\n",
    "if (fn):\n",
    "    confmat['FN'] = fn['count'] * 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9632324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics based on the confusion matrix:\n",
      " Linear Accuracy = 0.8768661210107273\n",
      " Linear Precision = 0.8258252679266906\n",
      " Linear Recall = 0.6504811404916131\n",
      " Linear Specifity = 0.9535365151699711\n",
      " Linear F1 score = 0.7277402022568982\n"
     ]
    }
   ],
   "source": [
    "# Based on the confusion matrix, computed the evaluation matrics:\n",
    "#   accuracy, precision, recall, specifity and F1 score\n",
    "\n",
    "# PS: Check divisons by 0.0\n",
    "TP = confmat['TP']\n",
    "TN = confmat['TN']\n",
    "FP = confmat['FP']\n",
    "FN = confmat['FN']\n",
    "\n",
    "accuracy_linear = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0.0\n",
    "precision_linear = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "recall_linear = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "specifity_linear = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "f1score_linear = 2 * precision_linear * recall_linear / (precision_linear + recall_linear) if (precision_linear + recall_linear) > 0 else 0.0\n",
    "\n",
    "print('Evaluation metrics based on the confusion matrix:')\n",
    "print(f' Linear Accuracy = {accuracy_linear}')\n",
    "print(f' Linear Precision = {precision_linear}')\n",
    "print(f' Linear Recall = {recall_linear}')\n",
    "print(f' Linear Specifity = {specifity_linear}')\n",
    "print(f' Linear F1 score = {f1score_linear}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc5e42",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8c3fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg = PipelineModel.load('model-LogReg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62e4a83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric areaUnderROC of the Logistic Regression model = 0.8984182843426602\n"
     ]
    }
   ],
   "source": [
    "df_predictions = model_logreg.transform(df_validation)\n",
    "\n",
    "df_predictions_eval = df_predictions.select('features', \n",
    "                    'rawPrediction', 'prediction', 'Arrest')\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol='Arrest',\n",
    "                                                rawPredictionCol='rawPrediction',\n",
    "                                                metricName='areaUnderROC')\n",
    "    \n",
    "area_under_ROC_logreg = binary_evaluator.evaluate(df_predictions_eval)\n",
    "\n",
    "# Print out result\n",
    "print(f'Metric areaUnderROC of the Logistic Regression model = {area_under_ROC_logreg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "876de105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+\n",
      "|prediction|Arrest|  count|\n",
      "+----------+------+-------+\n",
      "|       0.0|     0|1622855|\n",
      "|       1.0|     1| 355287|\n",
      "|       1.0|     0|  51901|\n",
      "|       0.0|     1| 211907|\n",
      "+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting of the kind of predictions made\n",
    "df_confusion_matrix = df_predictions_eval.groupBy('prediction', 'Arrest').count()\n",
    "df_confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3370d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "tp = df_confusion_matrix.filter((F.col('prediction')==1.0) & (F.col('Arrest')==1)).first()\n",
    "tn = df_confusion_matrix.filter((F.col('prediction')==0.0) & (F.col('Arrest')==0)).first()\n",
    "fp = df_confusion_matrix.filter((F.col('prediction')==1.0) & (F.col('Arrest')==0)).first()\n",
    "fn = df_confusion_matrix.filter((F.col('prediction')==0.0) & (F.col('Arrest')==1)).first()\n",
    "\n",
    "confmat = {'TP': 0.0, 'TN': 0.0, 'FP': 0.0, 'FN': 0.0}\n",
    "if (tp):\n",
    "    confmat['TP'] = tp['count'] * 1.0\n",
    "if (tn):\n",
    "    confmat['TN'] = tn['count'] * 1.0\n",
    "if (fp):\n",
    "    confmat['FP'] = fp['count'] * 1.0\n",
    "if (fn):\n",
    "    confmat['FN'] = fn['count'] * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18d7b84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics based on the confusion matrix:\n",
      " Logistic Regression Accuracy = 0.882331006489886\n",
      " Logistic Regression Precision = 0.8725379922787508\n",
      " Logistic Regression Recall = 0.6263941438026496\n",
      " Logistic Regression Specifity = 0.9690098139669301\n",
      " Logistic Regression F1 score = 0.7292560823167916\n"
     ]
    }
   ],
   "source": [
    "# Based on the confusion matrix, computed the evaluation matrics:\n",
    "#   accuracy, precision, recall, specifity and F1 score\n",
    "\n",
    "# PS: Check divisons by 0.0\n",
    "TP = confmat['TP']\n",
    "TN = confmat['TN']\n",
    "FP = confmat['FP']\n",
    "FN = confmat['FN']\n",
    "\n",
    "accuracy_logreg = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0.0\n",
    "precision_logreg = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "recall_logreg = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "specifity_logreg = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "f1score_logreg = 2 * precision_logreg * recall_logreg / (precision_logreg + recall_logreg) if (precision_logreg + recall_logreg) > 0 else 0.0\n",
    "\n",
    "print('Evaluation metrics based on the confusion matrix:')\n",
    "print(f' Logistic Regression Accuracy = {accuracy_logreg}')\n",
    "print(f' Logistic Regression Precision = {precision_logreg}')\n",
    "print(f' Logistic Regression Recall = {recall_logreg}')\n",
    "print(f' Logistic Regression Specifity = {specifity_logreg}')\n",
    "print(f' Logistic Regression F1 score = {f1score_logreg}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4f46a",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6072b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = PipelineModel.load('model-RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb014b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric areaUnderROC of the Random Forest model = 0.8645578453101262\n"
     ]
    }
   ],
   "source": [
    "df_predictions = model_rf.transform(df_validation)\n",
    "\n",
    "df_predictions_eval = df_predictions.select('features', \n",
    "                    'rawPrediction', 'prediction', 'Arrest')\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol='Arrest',\n",
    "                                                rawPredictionCol='rawPrediction',\n",
    "                                                metricName='areaUnderROC')\n",
    "    \n",
    "area_under_ROC_rf = binary_evaluator.evaluate(df_predictions_eval)\n",
    "\n",
    "# Print out result\n",
    "print(f'Metric areaUnderROC of the Random Forest model = {area_under_ROC_rf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33a6fa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+\n",
      "|prediction|Arrest|  count|\n",
      "+----------+------+-------+\n",
      "|       0.0|     0|1674395|\n",
      "|       1.0|     1|  65633|\n",
      "|       1.0|     0|    361|\n",
      "|       0.0|     1| 501561|\n",
      "+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting of the kind of predictions made\n",
    "df_confusion_matrix = df_predictions_eval.groupBy('prediction', 'Arrest').count()\n",
    "df_confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35c9c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "tp = df_confusion_matrix.filter((F.col('prediction')==1.0) & (F.col('Arrest')==1)).first()\n",
    "tn = df_confusion_matrix.filter((F.col('prediction')==0.0) & (F.col('Arrest')==0)).first()\n",
    "fp = df_confusion_matrix.filter((F.col('prediction')==1.0) & (F.col('Arrest')==0)).first()\n",
    "fn = df_confusion_matrix.filter((F.col('prediction')==0.0) & (F.col('Arrest')==1)).first()\n",
    "\n",
    "confmat = {'TP': 0.0, 'TN': 0.0, 'FP': 0.0, 'FN': 0.0}\n",
    "if (tp):\n",
    "    confmat['TP'] = tp['count'] * 1.0\n",
    "if (tn):\n",
    "    confmat['TN'] = tn['count'] * 1.0\n",
    "if (fp):\n",
    "    confmat['FP'] = fp['count'] * 1.0\n",
    "if (fn):\n",
    "    confmat['FN'] = fn['count'] * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26b548c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics based on the confusion matrix:\n",
      " Random Forest Accuracy = 0.7761225718682397\n",
      " Random Forest Precision = 0.9945298057399158\n",
      " Random Forest Recall = 0.11571525791880732\n",
      " Random Forest Specifity = 0.9997844462118661\n",
      " Random Forest F1 score = 0.20730967737859846\n"
     ]
    }
   ],
   "source": [
    "# Based on the confusion matrix, computed the evaluation matrics:\n",
    "#   accuracy, precision, recall, specifity and F1 score\n",
    "\n",
    "# PS: Check divisons by 0.0\n",
    "TP = confmat['TP']\n",
    "TN = confmat['TN']\n",
    "FP = confmat['FP']\n",
    "FN = confmat['FN']\n",
    "\n",
    "accuracy_rf = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0.0\n",
    "precision_rf = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "recall_rf = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "specifity_rf = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "f1score_rf = 2 * precision_rf * recall_rf / (precision_rf + recall_rf) if (precision_rf + recall_rf) > 0 else 0.0\n",
    "\n",
    "print('Evaluation metrics based on the confusion matrix:')\n",
    "print(f' Random Forest Accuracy = {accuracy_rf}')\n",
    "print(f' Random Forest Precision = {precision_rf}')\n",
    "print(f' Random Forest Recall = {recall_rf}')\n",
    "print(f' Random Forest Specifity = {specifity_rf}')\n",
    "print(f' Random Forest F1 score = {f1score_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab215a9",
   "metadata": {},
   "source": [
    "Now to compare the 3 different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84050e97",
   "metadata": {},
   "source": [
    "Lookin at Random Forest's recall, we can see its very low so we are excludingit from consideration right away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ece0c3",
   "metadata": {},
   "source": [
    "Now, comparing logistic regression and linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d61588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Linear Accuracy = 0.8768661210107273\n",
      " Logistic Regression Accuracy = 0.882331006489886\n",
      " \n",
      " Linear Precision = 0.8258252679266906\n",
      " Logistic Regression Precision = 0.8725379922787508\n",
      " \n",
      " Linear Recall = 0.6504811404916131\n",
      " Logistic Regression Recall = 0.6263941438026496\n",
      " \n",
      " Linear Specifity = 0.9535365151699711\n",
      " Logistic Regression Specifity = 0.9690098139669301\n",
      " \n",
      " Linear F1 score = 0.7277402022568982\n",
      " Logistic Regression F1 score = 0.7292560823167916\n",
      " \n",
      "Metric areaUnderROC of the linear model = 0.8672023696346972\n",
      "Metric areaUnderROC of the Logistic Regression model = 0.8984182843426602\n"
     ]
    }
   ],
   "source": [
    "print(f' Linear Accuracy = {accuracy_linear}')\n",
    "print(f' Logistic Regression Accuracy = {accuracy_logreg}')\n",
    "print(\" \")\n",
    "print(f' Linear Precision = {precision_linear}')\n",
    "print(f' Logistic Regression Precision = {precision_logreg}')\n",
    "print(\" \")\n",
    "print(f' Linear Recall = {recall_linear}')\n",
    "print(f' Logistic Regression Recall = {recall_logreg}')\n",
    "print(\" \")\n",
    "print(f' Linear Specifity = {specifity_linear}')\n",
    "print(f' Logistic Regression Specifity = {specifity_logreg}')\n",
    "print(\" \")\n",
    "print(f' Linear F1 score = {f1score_linear}')\n",
    "print(f' Logistic Regression F1 score = {f1score_logreg}')\n",
    "print(\" \")\n",
    "print(f'Metric areaUnderROC of the linear model = {area_under_ROC_linear}')\n",
    "print(f'Metric areaUnderROC of the Logistic Regression model = {area_under_ROC_logreg}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eaa148",
   "metadata": {},
   "source": [
    "As we can see, they are pretty similar so we'll go foward with the Linear model since it was the one we used in class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
