{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969ccf3a",
   "metadata": {},
   "source": [
    "## Reading the previously analized and prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13594bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LinearSVC, LogisticRegression, GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10128e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimesFix\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0428f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "data_dir = '../Datasets/'\n",
    "file_crimes = data_dir + '3_crimes_cleaned'\n",
    "df_clean = spark.read.parquet(file_crimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "846fc3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean dataframe - number of rows: 2300084\n",
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- IUCR_Num: integer (nullable = true)\n",
      " |-- Primary_Type_Num: integer (nullable = true)\n",
      " |-- Location_Description_Num: integer (nullable = true)\n",
      " |-- Arrest: integer (nullable = true)\n",
      " |-- Domestic: integer (nullable = true)\n",
      " |-- Beat: integer (nullable = true)\n",
      " |-- District: integer (nullable = true)\n",
      " |-- Ward: integer (nullable = true)\n",
      " |-- Community_Area: integer (nullable = true)\n",
      " |-- FBI_Code_Num: integer (nullable = true)\n",
      "\n",
      "+----+-----+---+----+------+--------+----------------+------------------------+------+--------+----+--------+----+--------------+------------+\n",
      "|Year|Month|Day|Hour|Minute|IUCR_Num|Primary_Type_Num|Location_Description_Num|Arrest|Domestic|Beat|District|Ward|Community_Area|FBI_Code_Num|\n",
      "+----+-----+---+----+------+--------+----------------+------------------------+------+--------+----+--------+----+--------------+------------+\n",
      "|2018|   10| 10|   7|     0|      58|              17|                       0|     1|       0|1111|      11|  37|            23|          19|\n",
      "|2001|    3|  3|  12|    30|      58|              17|                     109|     1|       0| 314|       3|  20|            42|          19|\n",
      "|2001|    1|  9|  13|    30|      58|              17|                       0|     1|       0| 424|       4|   7|            46|          19|\n",
      "|2001|    1| 12|   0|    42|      18|              11|                       0|     0|       0|1231|      12|   2|            28|          12|\n",
      "|2001|    1| 16|   1|    17|      58|              17|                       8|     1|       0|1111|      11|  28|            25|          19|\n",
      "|2001|    1| 26|  17|     0|       3|               0|                      39|     0|       0|1434|      14|  32|            24|           0|\n",
      "|2001|    2|  5|  10|    45|       0|               1|                       2|     0|       1|1511|      15|  29|            25|           1|\n",
      "|2001|    2| 16|  15|    15|       7|               6|                       5|     0|       0| 232|       2|   3|            37|           5|\n",
      "|2001|    2| 24|   1|    50|      58|              17|                     127|     0|       0|1412|      14|  35|            21|          19|\n",
      "|2001|    2| 24|  22|     0|      49|               6|                       0|     0|       0| 911|       9|  14|            63|           5|\n",
      "+----+-----+---+----+------+--------+----------------+------------------------+------+--------+----+--------+----+--------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data\n",
    "print(f'Clean dataframe - number of rows: {df_clean.count()}')\n",
    "df_clean = df_clean.drop('IUCR', 'Primary_Type', 'Location_Description', 'FBI_Code')\n",
    "df_clean.printSchema()\n",
    "df_clean.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74177ae1",
   "metadata": {},
   "source": [
    "## Feature engineering for ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701295e",
   "metadata": {},
   "source": [
    "### Create lists of categorical and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbc87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['IUCR_Num', 'Primary_Type_Num', 'Location_Description_Num', 'Domestic', 'Beat', 'District', 'Ward', 'Community_Area', 'FBI_Code_Num']\n",
      "Numeric columns: ['Year', 'Month', 'Day', 'Hour', 'Minute']\n",
      "OHE columns: ['IUCR_Num_OHE', 'Primary_Type_Num_OHE', 'Location_Description_Num_OHE', 'Domestic_OHE', 'Beat_OHE', 'District_OHE', 'Ward_OHE', 'Community_Area_OHE', 'FBI_Code_Num_OHE']\n",
      "Assembler inputs: ['IUCR_Num_OHE', 'Primary_Type_Num_OHE', 'Location_Description_Num_OHE', 'Domestic_OHE', 'Beat_OHE', 'District_OHE', 'Ward_OHE', 'Community_Area_OHE', 'FBI_Code_Num_OHE', 'Year', 'Month', 'Day', 'Hour', 'Minute']\n"
     ]
    }
   ],
   "source": [
    "# Categorical attributes\n",
    "# (will be used for OneHotEncoder)\n",
    "categorical_cols = [\n",
    "    'IUCR_Num', 'Primary_Type_Num', 'Location_Description_Num', 'Domestic',\n",
    "    'Beat', 'District', 'Ward', 'Community_Area', 'FBI_Code_Num'\n",
    "]\n",
    "\n",
    "numeric_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']\n",
    "\n",
    "# Target column\n",
    "target_col = 'Arrest'\n",
    "\n",
    "# Column names for OneHotEncoder output\n",
    "ohe_output_cols = [f'{c}_OHE' for c in categorical_cols]\n",
    "\n",
    "# OneHotEncoder for categorical features\n",
    "ohe_encoder = OneHotEncoder(\n",
    "    inputCols=categorical_cols,\n",
    "    outputCols=ohe_output_cols,\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Collecting all the features for ML model into a vector\n",
    "assembler_inputs = ohe_output_cols + numeric_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(\n",
    "    inputCols=assembler_inputs,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Checking the columns\n",
    "print(f'Categorical columns: {categorical_cols}')\n",
    "print(f'Numeric columns: {numeric_cols}')\n",
    "print(f'OHE columns: {ohe_output_cols}')\n",
    "print(f'Assembler inputs: {assembler_inputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456744a4",
   "metadata": {},
   "source": [
    "### Splitting the  dataset into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df029fd6",
   "metadata": {},
   "source": [
    "Before splitting the dataset, it is important to check the distribution of the target variable to ensure that the classes are reasonably balanced. \\\n",
    "This helps prevent the model from becoming biased toward the majority class and ensures it can effectively learn to predict both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74f2afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------------+\n",
      "|Arrest|  count|        percentage|\n",
      "+------+-------+------------------+\n",
      "|     0|1779396| 77.36221807551377|\n",
      "|     1| 520688|22.637781924486237|\n",
      "+------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = df_clean.count()\n",
    "\n",
    "df_clean.groupBy('Arrest') \\\n",
    "    .agg(\n",
    "        F.count('*').alias('count'),\n",
    "        (F.count('*') / total_count * 100).alias('percentage')\n",
    "    ) \\\n",
    "    .orderBy('Arrest') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae71b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1609952 rows in the training set and 690132 rows in the validation set.\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df_clean.randomSplit([0.7, 0.3], 42)\n",
    "\n",
    "print(f'There are {df_train.count()} rows in the training set and {df_test.count()} rows in the validation set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f7bdf",
   "metadata": {},
   "source": [
    "The target variable `Arrest` is imbalanced (77% — class 0, 23% — class 1). To prevent the model from being biased towards the majority class, class weights were applied. \\\n",
    "This improves the model’s ability to correctly predict both classes without losing data or causing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c53e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 count: 1245762, Class 1 count: 364190\n",
      "Balance ratio: 3.420637579285538\n"
     ]
    }
   ],
   "source": [
    "# Calculating class balance\n",
    "count_0 = df_train.filter(df_train.Arrest == 0).count()\n",
    "count_1 = df_train.filter(df_train.Arrest == 1).count()\n",
    "\n",
    "balance_ratio = count_0 / count_1\n",
    "\n",
    "print(f'Class 0 count: {count_0}, Class 1 count: {count_1}')\n",
    "print(f'Balance ratio: {balance_ratio}')\n",
    "\n",
    "# Adding a column with weights\n",
    "df_train_weighted = df_train.withColumn(\n",
    "    \"classWeightCol\",\n",
    "    F.when(F.col(\"Arrest\") == 1, balance_ratio).otherwise(1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7775ad",
   "metadata": {},
   "source": [
    "### Create parquet files with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfbc61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_weighted.write.mode('overwrite').parquet('../Datasets/crimes-small-train')\n",
    "df_test.write.mode('overwrite').parquet('../Datasets/crimes-small-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c72a1",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570fd727",
   "metadata": {},
   "source": [
    "### Linear SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99783f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM model initialization\n",
    "lsvc = LinearSVC(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    labelCol=\"Arrest\",\n",
    "    weightCol=\"classWeightCol\"\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[ohe_encoder, vec_assembler, lsvc])\n",
    "\n",
    "# Saving the pipeline\n",
    "pipeline.write().overwrite().save(\"../Datasets/pipeline-LinearSVM\")\n",
    "\n",
    "# Fitting the model\n",
    "model = pipeline.fit(df_train_weighted)\n",
    "\n",
    "# Saving the model\n",
    "model.write().overwrite().save(\"model-LinearSVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ccbc82",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37ebf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(\n",
    "    maxIter=10, \n",
    "    regParam=0.1, \n",
    "    labelCol='Arrest', \n",
    "    weightCol=\"classWeightCol\"\n",
    ")\n",
    "pipeline_logreg = Pipeline(stages=[ohe_encoder, vec_assembler, logreg])\n",
    "pipeline_logreg.write().overwrite().save('../Datasets/pipeline-LogReg')\n",
    "model_logreg = pipeline_logreg.fit(df_train_weighted)\n",
    "model_logreg.write().overwrite().save('model-LogReg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca89f2",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f2e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    labelCol='Arrest', \n",
    "    featuresCol='features', \n",
    "    numTrees=100,\n",
    "    weightCol=\"classWeightCol\"\n",
    ")\n",
    "pipeline_rf = Pipeline(stages=[ohe_encoder, vec_assembler, rf])\n",
    "pipeline_rf.write().overwrite().save('../Datasets/pipeline-RandomForest')\n",
    "model_rf = pipeline_rf.fit(df_train_weighted)\n",
    "model_rf.write().overwrite().save('model-RandomForest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
