{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Algoritmos para Big Data\n",
    "\n",
    "**Handout 5 - Apache Kafka as a messaging system, and data profiling**\n",
    "\n",
    "**2024/25**\n",
    "\n",
    "This lab class aims to introduce Apache Kafka as a messaging system that may play an important role in data streaming. Additionally, profiling of data to be used is considered, based upon reports generated by a specific tool -- YData Profiling.\n",
    "\n",
    "This is one of the notebooks that should contain the implementation of the tasks presented in the handout. This notebook is about data cleaning/preparation regarding the dataset provided so it uses the YData Profiling tool.\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "The data file to work with can be downloaded from the zip archive located at:\n",
    "\n",
    "https://bigdata.iscte-iul.eu/datasets/books-amazon.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataPreparation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task A - Data ingestion\n",
    "\n",
    "**Reading and checking data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ASIN\",\"GROUP\",\"FORMAT\",\"TITLE\",\"AUTHOR\",\"PUBLISHER\"\n",
      "\"1250150183\",\"book\",\"hardcover\",\"The Swamp: Washington's Murky Pool of Corruption and Cronyism and How Trump Can Drain It\",\"Eric Bolling\",\"St. Martin's Press\"\n",
      "\"0778319997\",\"book\",\"hardcover\",\"Rise and Shine, Benedict Stone: A Novel\",\"Phaedra Patrick\",\"Park Row Books\"\n",
      "\"1608322564\",\"book\",\"hardcover\",\"Sell or Be Sold: How to Get Your Way in Business and in Life\",\"Grant Cardone\",\"Greenleaf Book Group Press\"\n",
      "\"0310325331\",\"book\",\"hardcover\",\"Christian Apologetics: An Anthology of Primary Sources\",\"Khaldoun A. Sweis, Chad V. Meister\",\"Zondervan\"\n",
      "\"0312616295\",\"book\",\"hardcover\",\"Gravity: How the Weakest Force in the Universe Shaped Our Lives\",\"Brian Clegg\",\"St. Martin's Press\"\n",
      "\"1250066190\",\"book\",\"hardcover\",\"Glass Houses: A Novel (Chief Inspector Gamache Novel)\",\"Louise Penny\",\"Minotaur Books\"\n",
      "\"1592643124\",\"book\",\"hardcover\",\"Reference Guide to the Talmud\",\"Rabbi Adin Steinsaltz\",\"The Toby Press\"\n",
      "\"1849962839\",\"book\",\"hardcover\",\"Induction Motor Control Design (Advances in Industrial Control)\",\"Riccardo Marino, Patrizio Tomei, Cristiano M. Verrelli\",\"Springer\"\n",
      "\"022640014X\",\"book\",\"hardcover\",\"The Diversity Bargain: And Other Dilemmas of Race, Admissions, and Meritocracy at Elite Universities\",\"Natasha K. Warikoo\",\"University Of Chicago Press\"\n"
     ]
    }
   ],
   "source": [
    "# Data to read\n",
    "data_dir = '../Datasets/BooksAmazon/'\n",
    "data_file = data_dir + 'books-amazon.csv'\n",
    "\n",
    "! head $data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "df = spark.read.csv(\n",
    "        # data_dir,\n",
    "        data_file, \n",
    "        header=True, sep=',', inferSchema=True, \n",
    "        #recursiveFileLookup=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df - number of rows: 63755\n",
      "root\n",
      " |-- ASIN: string (nullable = true)\n",
      " |-- GROUP: string (nullable = true)\n",
      " |-- FORMAT: string (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- AUTHOR: string (nullable = true)\n",
      " |-- PUBLISHER: string (nullable = true)\n",
      "\n",
      "+----------+-----+---------+----------------------------------------------------------------------------------------------------+------------------------------------------------------+---------------------------+\n",
      "|ASIN      |GROUP|FORMAT   |TITLE                                                                                               |AUTHOR                                                |PUBLISHER                  |\n",
      "+----------+-----+---------+----------------------------------------------------------------------------------------------------+------------------------------------------------------+---------------------------+\n",
      "|1250150183|book |hardcover|The Swamp: Washington's Murky Pool of Corruption and Cronyism and How Trump Can Drain It            |Eric Bolling                                          |St. Martin's Press         |\n",
      "|0778319997|book |hardcover|Rise and Shine, Benedict Stone: A Novel                                                             |Phaedra Patrick                                       |Park Row Books             |\n",
      "|1608322564|book |hardcover|Sell or Be Sold: How to Get Your Way in Business and in Life                                        |Grant Cardone                                         |Greenleaf Book Group Press |\n",
      "|0310325331|book |hardcover|Christian Apologetics: An Anthology of Primary Sources                                              |Khaldoun A. Sweis, Chad V. Meister                    |Zondervan                  |\n",
      "|0312616295|book |hardcover|Gravity: How the Weakest Force in the Universe Shaped Our Lives                                     |Brian Clegg                                           |St. Martin's Press         |\n",
      "|1250066190|book |hardcover|Glass Houses: A Novel (Chief Inspector Gamache Novel)                                               |Louise Penny                                          |Minotaur Books             |\n",
      "|1592643124|book |hardcover|Reference Guide to the Talmud                                                                       |Rabbi Adin Steinsaltz                                 |The Toby Press             |\n",
      "|1849962839|book |hardcover|Induction Motor Control Design (Advances in Industrial Control)                                     |Riccardo Marino, Patrizio Tomei, Cristiano M. Verrelli|Springer                   |\n",
      "|022640014X|book |hardcover|The Diversity Bargain: And Other Dilemmas of Race, Admissions, and Meritocracy at Elite Universities|Natasha K. Warikoo                                    |University Of Chicago Press|\n",
      "|9176374343|book |hardcover|White Fang (Wisehouse Classics - With Original Illustrations)                                       |Jack London                                           |Wisehouse Classics         |\n",
      "+----------+-----+---------+----------------------------------------------------------------------------------------------------+------------------------------------------------------+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data that has been read\n",
    "print(f'df - number of rows: {df.count()}')\n",
    "df.printSchema()\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task B - Data profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df - number of rows is 63755; after dropDuplicates() applied would be 63750.\n"
     ]
    }
   ],
   "source": [
    "print(f'df - number of rows is {df.count()}; after dropDuplicates() applied would be {df.dropDuplicates().count()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking NULLs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df - number of rows after dropna(how='any') applied would be 57196.\n"
     ]
    }
   ],
   "source": [
    "print(f'''df - number of rows after dropna(how='any') applied would be {df.dropna(how='any').count()}.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking nulls at each column of df...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ASIN': 0,\n",
       " 'GROUP': 4,\n",
       " 'FORMAT': 5,\n",
       " 'TITLE': 8,\n",
       " 'AUTHOR': 83,\n",
       " 'PUBLISHER': 6492}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If it is not a huge DataFrame...\n",
    "print('Checking nulls at each column of df...')\n",
    "dict_nulls_df = {col: df.filter(df[col].isNull()).count() for col in df.columns}\n",
    "dict_nulls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data profiling with YData Profiling**\n",
    "\n",
    "More adequate data profile for large datasets with YData Profiling\n",
    "\n",
    "https://docs.profiling.ydata.ai/latest/\n",
    "\n",
    "Even with a dataset with a large number of rows, ydata-profiling is able \n",
    "to help as it supports both Pandas Dataframes and Spark Dataframes.\n",
    "\n",
    "See https://docs.profiling.ydata.ai/latest/integrations/pyspark/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile_title = 'books-amazon.csv'\n",
    "\n",
    "profile_report = ProfileReport(\n",
    "    df,\n",
    "    title=profile_title,\n",
    "    infer_dtypes=False,\n",
    "    interactions=None,\n",
    "    missing_diagrams=None,\n",
    "    correlations={\n",
    "        \"auto\": {\"calculate\": False},\n",
    "        \"pearson\": {\"calculate\": False},\n",
    "        \"spearman\": {\"calculate\": False},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the profile report as html\n",
    "# profile_report_html = profile_report.to_html()\n",
    "\n",
    "# Export the profile report as json\n",
    "# profile_report_json = profile_report.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9393b78575b4ac3ae0b6da7b524cb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbb08193cc24284bafb9a31ecd02d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14a7bd45de5424fb7cca9c91706fa30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9fcfeeeb2c4581b305258c46f607dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../Datasets/BooksAmazon/profile-books-amazon.csv.html'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_report_file = data_dir + 'profile-' + profile_title + '.html'\n",
    "profile_report.to_file(Path(profile_report_file))\n",
    "profile_report_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the profile report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task C - Data cleaning\n",
    "\n",
    "- Applying cleaning operations upon the raw data\n",
    "- Store the outcome in a file (the cleaned data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, check again the profile report**\n",
    "\n",
    "If needed and/or advisable, carry out cleaning operations over the DataFrame and store the outcome as a new data file. It can be stored as, for example:\n",
    "\n",
    "- a parquet file, which is the correct strategy at this stage;\n",
    "- or, if one wants, a csv file.\n",
    "\n",
    "Ultimately, make sure that the final data file is ready to be properly used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO AS APPROPRIATE\n",
    "\n",
    "# e.g. duplicates, missing data, etc.\n",
    "\n",
    "# df_clean = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS. Why repartition(1)? \n",
    "# And what about the directory/file to be created?\n",
    "\n",
    "# sep=';'\n",
    "# out_file = data_dir + 'credit-card-transactions.csv'\n",
    "# ( df_clean.repartition(1).write.mode('overwrite')\n",
    "#   .options(header=True, delimiter=sep)\n",
    "#   .csv(out_file)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
