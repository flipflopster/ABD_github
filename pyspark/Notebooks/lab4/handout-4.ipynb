{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Algoritmos para Big Data\n",
    "\n",
    "**Handout 3 - Feature extraction and transformation**\n",
    "\n",
    "**2024/25**\n",
    "\n",
    "This lab class will focus on feature extractors and transformers, which are are critical components in the field of machine learning and \n",
    "also for data preprocessing.\n",
    "\n",
    "This notebook should contain the implementation of the tasks presented in the handout.\n",
    "\n",
    "Hence both handout and notebook must be considered together as one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task A - Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasest**\n",
    "\n",
    "Recall that the file can be downloaded from \n",
    "\n",
    "https://bigdata.iscte-iul.eu/datasets/iot-devices.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SparkSession\n",
    "spark = SparkSession.builder.appName('Features').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading and checking data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User,Card,Year,Month,Day,Time,Amount,Use Chip,Merchant Name,Merchant City,Merchant State,Zip,MCC,Errors?,Is Fraud?\n",
      "0,0,2002,9,1,06:21,$134.09,Swipe Transaction,3527213246127876953,La Verne,CA,91750.0,5300,,No\n",
      "0,0,2002,9,1,06:42,$38.48,Swipe Transaction,-727612092139916043,Monterey Park,CA,91754.0,5411,,No\n",
      "0,0,2002,9,2,06:22,$120.34,Swipe Transaction,-727612092139916043,Monterey Park,CA,91754.0,5411,,No\n",
      "0,0,2002,9,2,17:45,$128.95,Swipe Transaction,3414527459579106770,Monterey Park,CA,91754.0,5651,,No\n",
      "0,0,2002,9,3,06:23,$104.71,Swipe Transaction,5817218446178736267,La Verne,CA,91750.0,5912,,No\n",
      "0,0,2002,9,3,13:53,$86.19,Swipe Transaction,-7146670748125200898,Monterey Park,CA,91755.0,5970,,No\n",
      "0,0,2002,9,4,05:51,$93.84,Swipe Transaction,-727612092139916043,Monterey Park,CA,91754.0,5411,,No\n",
      "0,0,2002,9,4,06:09,$123.50,Swipe Transaction,-727612092139916043,Monterey Park,CA,91754.0,5411,,No\n",
      "0,0,2002,9,5,06:14,$61.72,Swipe Transaction,-727612092139916043,Monterey Park,CA,91754.0,5411,,No\n"
     ]
    }
   ],
   "source": [
    "# Reading data\n",
    "data_dir = '../../Datasets/'\n",
    "file_iot = data_dir + 'credit-cards-transactions.csv'\n",
    "\n",
    "! head $file_iot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cards = spark.read.csv(file_iot, header=True, sep=',', inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+---+-------------------+-------+-----------------+--------------------+-------------+--------------+-------+----+-------+---------+\n",
      "|User|Card|Year|Month|Day|Time               |Amount |Use Chip         |Merchant Name       |Merchant City|Merchant State|Zip    |MCC |Errors?|Is Fraud?|\n",
      "+----+----+----+-----+---+-------------------+-------+-----------------+--------------------+-------------+--------------+-------+----+-------+---------+\n",
      "|0   |0   |2002|9    |1  |2025-03-27 06:21:00|$134.09|Swipe Transaction|3527213246127876953 |La Verne     |CA            |91750.0|5300|NULL   |No       |\n",
      "|0   |0   |2002|9    |1  |2025-03-27 06:42:00|$38.48 |Swipe Transaction|-727612092139916043 |Monterey Park|CA            |91754.0|5411|NULL   |No       |\n",
      "|0   |0   |2002|9    |2  |2025-03-27 06:22:00|$120.34|Swipe Transaction|-727612092139916043 |Monterey Park|CA            |91754.0|5411|NULL   |No       |\n",
      "|0   |0   |2002|9    |2  |2025-03-27 17:45:00|$128.95|Swipe Transaction|3414527459579106770 |Monterey Park|CA            |91754.0|5651|NULL   |No       |\n",
      "|0   |0   |2002|9    |3  |2025-03-27 06:23:00|$104.71|Swipe Transaction|5817218446178736267 |La Verne     |CA            |91750.0|5912|NULL   |No       |\n",
      "|0   |0   |2002|9    |3  |2025-03-27 13:53:00|$86.19 |Swipe Transaction|-7146670748125200898|Monterey Park|CA            |91755.0|5970|NULL   |No       |\n",
      "|0   |0   |2002|9    |4  |2025-03-27 05:51:00|$93.84 |Swipe Transaction|-727612092139916043 |Monterey Park|CA            |91754.0|5411|NULL   |No       |\n",
      "|0   |0   |2002|9    |4  |2025-03-27 06:09:00|$123.50|Swipe Transaction|-727612092139916043 |Monterey Park|CA            |91754.0|5411|NULL   |No       |\n",
      "|0   |0   |2002|9    |5  |2025-03-27 06:14:00|$61.72 |Swipe Transaction|-727612092139916043 |Monterey Park|CA            |91754.0|5411|NULL   |No       |\n",
      "|0   |0   |2002|9    |5  |2025-03-27 09:35:00|$57.10 |Swipe Transaction|4055257078481058705 |La Verne     |CA            |91750.0|7538|NULL   |No       |\n",
      "+----+----+----+-----+---+-------------------+-------+-----------------+--------------------+-------------+--------------+-------+----+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "df_cards - number of rows: 24386900\n",
      "root\n",
      " |-- User: integer (nullable = true)\n",
      " |-- Card: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Amount: string (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: long (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- Merchant State: string (nullable = true)\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- MCC: integer (nullable = true)\n",
      " |-- Errors?: string (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking iot data\n",
    "df_cards.show(10,truncate=False)\n",
    "print(f'df_cards - number of rows: {df_cards.count()      }')\n",
    "df_cards.printSchema()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cards = df_cards.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'df_cards - number of rows is {df_cards.count()}; after dropDuplicates() applied would be {df_cards.dropDuplicates().count()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_iot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mdf_iot - number of rows after dropna(how=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) applied would be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf_iot\u001b[49m\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;250m     \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_iot' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'''df_iot - number of rows after dropna(how='any') applied would be {df_iot.dropna(how='any').count()     }.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking nulls at each column of df_iot')\n",
    "dict_nulls_iot = {col: df_iot.filter(df_iot[col].isNull()).count() for col in df_iot.columns}\n",
    "dict_nulls_iot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dataframe with columns of interest, as well as arrays with names of columns to look at later on\n",
    "\n",
    "# column 'device': use F.regexp_replace() on column 'device_name' (use F.col()), replacing '-' by ' '.\n",
    "# column 'device_words': use F.split() on column 'device_name' (use F.col()) by '-'\n",
    "\n",
    "df_devices = ( df_iot\n",
    "            .withColumn('device', F.regexp_replace(F.col('device_name'), '-', ' '))\n",
    "            .withColumn('device_words', F.split(F.col('device_name'), '-'))\n",
    "            .select('device_id','device', 'device_words', 'battery_level', 'c02_level', 'humidity', 'temp', 'cn')\n",
    ")\n",
    "df_devices.show(10,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# call describe on df_devices and show\n",
    "df_devices.describe().show()\n",
    "\n",
    "# numeric columns\n",
    "input_cols_num = ['battery_level', 'c02_level', 'humidity', 'temp']\n",
    "# string columns\n",
    "input_cols_str = ['cn']\n",
    "# all interest columns together\n",
    "input_cols_all = input_cols_num + input_cols_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots (histograms) to grasp data\n",
    "\n",
    "# df_devices\n",
    "# input_cols_num = ['battery_level', 'c02_level', 'humidity', 'temp']\n",
    "# input_cols_str = ['cn']\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "df_pandas = df_devices.toPandas()\n",
    "\n",
    "id = 2\n",
    "# col = 'cn'\n",
    "col = input_cols_num[id]\n",
    "fig = px.histogram(df_pandas, x=col)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task B - Basic statistics\n",
    "\n",
    "Applying the following statistical algorithms upon the dataframe of interest:\n",
    "- Correlation, with help of feature transformer VectorAssembler\n",
    "- Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations among numeric columns\n",
    "#\n",
    "# Correlation needs vectors so we convert to vector column first\n",
    "# See VectorAssembler\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# The columns to compute correlations - numeric types but no nulls\n",
    "cols_corr = input_cols_num\n",
    "\n",
    "# Assemble columns\n",
    "col_features = \"features\"\n",
    "assembler = VectorAssembler(inputCols=cols_corr, outputCol=col_features, handleInvalid = \"skip\")\n",
    "# Apply transform on df_devices an select col_futures \n",
    "df_features = assembler.transform(df_devices)\n",
    "    \n",
    "\n",
    "df_features.show(10, truncate=False)\n",
    "\n",
    "# Get correlation matrix - it can be Pearson’s (default) or Spearman’s correlation\n",
    "corr_matrix = Correlation.corr(df_features, col_features).collect()[0][0].toArray().tolist()\n",
    "\n",
    "corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot computed correlation\n",
    "fig = px.imshow(corr_matrix, text_auto=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizer\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "summarizer = Summarizer.metrics('min', 'max', 'mean', 'sum', 'count', 'variance', 'std', 'normL1', 'normL2')\n",
    "\n",
    "df_features.show(10, truncate=False)\n",
    "\n",
    "print('Aggregated metric below:\\n')\n",
    "df_features.select(summarizer.summary(df_features.features)).show(truncate=False)\n",
    "\n",
    "print('Single metrics below:\\n')\n",
    "df_features.select(Summarizer.min(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.max(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.mean(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.sum(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.count(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.variance(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.std(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.normL2(df_features.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task C - Features extraction\n",
    "\n",
    "Extracting features frow raw data, according to the following algorithms:\n",
    "- TF-IDF\n",
    "- Word2Vec\n",
    "- CountVectorizer\n",
    "- FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_devices.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"device_words\", outputCol=\"rawFeatures\", numFeatures=10) \n",
    "hashing_df = hashingTF.transform(df_devices) \n",
    "\n",
    "hashing_df.show(10,truncate=False)\n",
    "\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"featuresTFIDF\") \n",
    "idf_model = idf.fit(hashing_df) \n",
    "\n",
    "tfidf_df = idf_model.transform(hashing_df) \n",
    "tfidf_df.select('device_id','device_words','featuresTFIDF').show(10,truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"device_words\", outputCol=\"features\")\n",
    "word2vec_model = word2vec.fit(df_devices)\n",
    "word2vec_df = word2vec_model.transform(df_devices)\n",
    "word2vec_df.select(\"device_id\", \"device_words\", \"features\").show(10,truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"device_words\", outputCol=\"features\", vocabSize=3) \n",
    "cv_model = cv.fit(df_devices)\n",
    "cv_df = cv_model.transform(df_devices)\n",
    "cv_df.select(\"device_id\",'device_words', \"features\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureHasher\n",
    "from pyspark.ml.feature import FeatureHasher \n",
    "\n",
    "hasher = FeatureHasher(inputCols=input_cols_all, outputCol='features', numFeatures=4) \n",
    "hasher_df = hasher.transform(df_devices)\n",
    "cols_to_show = ['device_id','device'] + input_cols_all + ['features']\n",
    "hasher_df.select(cols_to_show).show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task D - Feature transformation\n",
    "\n",
    "Modifying features into more suitable formats, according to the following algorithms: \n",
    "- Tokenizer\n",
    "- StringIndexer\n",
    "- OneHotEncoder\n",
    "- Binarizer\n",
    "- Bucketizer\n",
    "- StandardScaler\n",
    "- MinMaxScaler\n",
    "- PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='device', outputCol='device_tokens')\n",
    "df_tokenizer = tokenizer.\n",
    "df_tokenizer.select('device_id', 'device', 'device_words', 'device_tokens').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"cn\", outputCol=\"cn_indexed\") \n",
    "indexer_model = indexer.\n",
    "indexer_df = indexer_model.\n",
    "indexer_df.select('device_id', 'device', 'cn', 'cn_indexed').show(10, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "incols = ['battery_level', 'c02_level']\n",
    "outcols = ['battery_level_vec', 'c02_level_vec']\n",
    "encoder = OneHotEncoder(inputCols=incols,outputCols=outcols) \n",
    "encoder_model = encoder.\n",
    "encoder_df = encoder_model.\n",
    "cols_to_show = ['device_id', 'device'] + incols + outcols\n",
    "encoder_df.select(cols_to_show).show(10,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarizer\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "# As operator requires numeric data but not integer, we have to create a new but adequate column\n",
    "df = df_devices.select('device_id', 'device','battery_level').withColumn('battery_level_d', F.col('battery_level').cast('double'))\n",
    "\n",
    "threshold_cut = 5.0\n",
    "binarizer = Binarizer(threshold=threshold_cut, inputCol='battery_level_d', outputCol='battery_level_binary') \n",
    "binarizer_df = binarizer.\n",
    "print(f'Binarizer output with threshold {threshold_cut}:') \n",
    "binarizer_df.show(10, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketizer\n",
    "from pyspark.ml.feature import Bucketizer \n",
    " \n",
    "splits = [0.0, 2.0, 4.0, float('inf')] \n",
    "bucketizer = Bucketizer(splits=splits, inputCol='battery_level', outputCol='battery_level_bucket') \n",
    "bucketizer_df = bucketizer.\n",
    "print(f'Bucketizer output with {(len(bucketizer.getSplits()) - 1)} buckets:') \n",
    "bucketizer_df.select('device_id', 'device', 'battery_level', 'battery_level_bucket').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "from pyspark.ml.feature import StandardScaler \n",
    "\n",
    "# Assemble columns\n",
    "col_features = 'features'\n",
    "col_features_scaled = 'features_scaled' \n",
    "assembler = VectorAssembler(inputCols=input_cols_num, outputCol=col_features, handleInvalid = \"skip\") # \"keep\"\n",
    "df_features = assembler.\n",
    "\n",
    "scaler = StandardScaler(inputCol=col_features, outputCol=col_features_scaled, withStd=True, withMean=True) \n",
    "scaler_model = scaler.\n",
    "scaler_df = scaler_model.\n",
    "\n",
    "cols_to_show = ['device_id','device'] + input_cols_num + [col_features, col_features_scaled]\n",
    "scaler_df.select(cols_to_show).show(10,truncate=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "#col_features = 'features'\n",
    "#col_features_scaled = 'features_scaled' \n",
    "#assembler = VectorAssembler(inputCols=input_cols_num, outputCol=col_features, handleInvalid = \"skip\") # \"keep\"\n",
    "#df_features = assembler.transform(df_devices)\n",
    "\n",
    "# Use of col_features, col_features_scaled, and df_features \n",
    "# from previous exercise StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=col_features, outputCol=col_features_scaled) \n",
    "scaler_model = scaler.\n",
    "scaler_df = scaler_model.\n",
    "\n",
    "cols_to_show = ['device_id','device'] + input_cols_num + [col_features, col_features_scaled]\n",
    "print(f'Features scaled to range [{scaler.getMin()}, {scaler.getMax()}]:') \n",
    "scaler_df.select(cols_to_show).show(10,truncate=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA (Principal Component Analysis)\n",
    "from pyspark.ml.feature import PCA \n",
    "\n",
    "# Use of col_features and df_features \n",
    "# from previous exercise StandardScaler\n",
    "\n",
    "col_features_pca = 'features_PCA'\n",
    "pca = PCA(k=2, inputCol=col_features, outputCol=col_features_pca) \n",
    "pca_model = pca.\n",
    "pca_df = pca_model.\n",
    "\n",
    "cols_to_show = ['device_id','device'] + input_cols_num + [col_features, col_features_pca] \n",
    "pca_df.select(cols_to_show).show(10,truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task E - Feature selection\n",
    "\n",
    "Selecting a relevant subset of features from a larger set of features\n",
    "- ChiSqSelector\n",
    "- VectorSlicer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChiSqSelector\n",
    "from pyspark.ml.feature import ChiSqSelector \n",
    "\n",
    "# Use of col_features and df_features \n",
    "# from previous exercise StandardScaler\n",
    "\n",
    "# Add a label column, such as danger = 1 if battery_level < 2 || c02_level > 1000 || temp > 26; 0 otherwise\n",
    "df = df_features.withColumn('danger', \n",
    "            F.when(        , 1.0)\n",
    "            .otherwise(0.0)\n",
    "        )\n",
    "# df.show()\n",
    "col_selected_features = 'selected_features'\n",
    "col_label = 'danger'\n",
    "selector = ChiSqSelector(numTopFeatures=2, featuresCol=col_features, outputCol=col_selected_features, labelCol=col_label) \n",
    "selector_model = selector.\n",
    "selector_df = selector_model.\n",
    "\n",
    "cols_to_show = ['device_id','device'] + input_cols_num + [col_features, col_label, col_selected_features] \n",
    "selector_df.select(cols_to_show).show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorSlicer\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "\n",
    "# Use of col_features and df_features \n",
    "# from previous exercise StandardScaler\n",
    "\n",
    "col_selected_features = 'selected_features'\n",
    "# indices: humidity -> 2, temp -> 3\n",
    "slicer = VectorSlicer(inputCol=col_features, outputCol=col_selected_features, indices=[2,3]) \n",
    "slicer_df = slicer.\n",
    "\n",
    "cols_to_show = ['device_id','device'] + input_cols_num + [col_features, col_selected_features] \n",
    "slicer_df.select(cols_to_show).show(10,truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
